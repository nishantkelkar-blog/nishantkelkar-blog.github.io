---
layout: post
author: Nishant Kelkar
title: Design a Task Hiring System
tags: system-design
---

Design a task hiring system like TaskRabbit.
Users should be able to specify a task they want to hire for, browse a list of available workers to work on that task (along with pricing information), and eventually be able to submit a request for hire.

## Requirements gathering

In such a broadly scoped interview question, it is very important to narrow down the scope of the problem.
There's only so much you can do in 45 - 60 minutes.
This means you have to ask a lot of questions about the scope of the problem to your interviewer.
Here's some that you may decide to ask:

```text
1. What 2-3 specific functionalities can we focus on for this interview to design for this system?
2. How many daily active users (DAU) are there?
3. How many daily active workers are there?
4. Do we want to limit the workers a user sees based on geographical location (e.g. within 5 miles)?
5. How many different task types do we support?
6. Do we want to allow for users to chat with workers within the app?
7. Do we want to allow for users to leave reviews/stars/ratings/etc.?
```

These provide a good starting point for the design.
Let us assume the following answers to these questions:

```text
1. We want to support the following functions:
- Allow a user to browse a list of candidate workers. As a search parameter, we want to allow the user to specify their budget as a max price (/hr).
- Allow a user to submit a work order to a chosen worker (work order = a task type, short description, work date, and obfuscated user location).
- Allow a worker to accept or deny the work order. The outcome should be communicated to the user.

2. We have 10M DAU.
3. We have 1M daily active workers.
4. Yes, it makes sense to pair a worker with a user within the same geographical region.
5. To keep it simple, let's say 5: Handyman, Cleaning, Cooking, Heavylifting, and Roofer.
6. To keep it simple, we do not want to support chat on this platform.
7. To keep it simple, let's say we allow users to leave a rating of 1-5 for each worker.
```

As requirements, we now have:

```text
1. 10M DAU.
2. 1M daily active workers.
3. Support getCandidateWorkers(loc, max_price), submitOrder(worker_id, order),
   submitReview(user_id, worker_id, order_id, rating),
   submitResponse(worker_id, order_id, yes_or_no) APIs. No chat support required.
4. 5 work categories.
5. Each worker will have a rating score from each past work item (1-5).
```

## Data model and high-level design

In such an application, it is important to get an idea of the entities that will be interacting with each other.
At a high level, we have: User, Worker, WorkerServiceLocations, Orders.

```sql
-- 8 + 100 + 30 + 1000 ~= 1200 bytes/row
User:
- id BIGINT (autogenerated),
- name VARCHAR(100),
- location JSON,
- dp_img_path VARCHAR(1000),
...

-- 8 + 100 + 30 + 1000 + 8 + 8 ~= 1200 bytes/row
Worker:
- id BIGINT (autogenerated),
- name VARCHAR(100),
- base_location JSON,
- dp_img_path VARCHAR(1000),
- avg_rating DOUBLE,
- num_ratings INTEGER,
...

-- 8 + 8 + 30 + 8 + 8 + 8 ~= 100 bytes/row
-- 1:many w.r.t. workerid:#rows
WorkerServiceLocations:
- id BIGINT (autogenerated),
- workerid BIGINT,
- base_location JSON,
- radius INTEGER,
- type ENUM, -- User, Worker, WorkerServiceLocations, Orders
- charge_per_hr DOUBLE,
...

-- 8 + 8 + 8 + 8 + 8 + 8 ~= 50 bytes/row (not incl. description)
Orders:
- id BIGINT (autogenerated),
- userid BIGINT,
- workerid BIGINT,
- description VARCHAR,
- type ENUM,
- status ENUM, -- Submitted, Accepted, Rejected, Completed
- requested_date DATETIME,
...
```

<figure class="blog-fig">
  <img src="/assets/images/task-hiring-high-level.png">
  <figcaption>Figure 1. Super high level components view</figcaption>
</figure>

At a high-level, we want to have a setup similar to fig. 1 shown above.
It shows the primary actors in our application, i.e. users and workers.
We have a set of frontend web servers that validates the initial request from clients, authenticates, and acts as a middleware for our backend systems.
The frontend servers are stateless, which means we can scale them very easily by adding more servers in our fleet.
There is also a load balancer layer in front of these servers (not shown) that is in charge of distributing traffic across the fleet in a ~roughly equal manner.

We also have a search service that is in charge of finding good candidate workers based on the user's requirements.
Some criteria for ranking candidates returned are: worker has good average rating, worker supports the requested type of work, worker serves in the user's area, worker provides services within user budget, etc.
By default, the service returns a paginated list of best matched workers, where the page size can be something easy to deal with w.r.t. scrolling on phones (e.g. 10).

We have an orders processing service that is in charge of processing orders.
Both users as well as workers write to the backend DB via this service.
Users write new orders to the DB, whereas workers update existing orders (i.e. to indicate if they want to take up the order or not).
The backend entities DB stores the entities listed out above.

Finally, we have a notifications management sub-system.
Once there is an update on an order by a worker, it is communicated to the users via this system.

At this point, it is crucial to get buy-in from your interviewer to proceed with the deep dive.
Remember: NEVER proceed to the deep dive without an initial buy-in from your interviewer.

## Deep dive

What are the key items to tackle now?
We have a few:

1. What data storage system to use to support the entities DB?
2. What data storage system to use to support search services?
3. What do the request/responses look like between various components of the system for our use-cases?
4. How will the order update notifications work?

Firstly, in order to not divert ourselves from our primary goal here, we will not expand upon the frontend servers setup.
The summary from our high-level overview is sufficient.

Let us do some storage calculations.
We have 10M DAU, and assuming an 'active user' here means submits at least 1 work order, we have 10M daily work orders.
Assuming each order requires 100KB (very likely a 2-3x overestimation, but possible since we allow for free-form text descriptions), we require a daily storage of:

10M x 100KB ~= **1TB storage/day**.

Planning for a 5 year retention, we need 1TB x 365 x 5 ~= **2 PB** storage.

This is massive for a RDBMS to handle without setting up signficant optimizations (caching, sharding, shard indexes, etc.).

It is better to store this order data in a fast key/value store, where it is easy to scale storage for.
The key here is the `order_id`, and value is the `description` field, the largest possible section within the `Orders` entity.

The rest of the order information can be stored in an RDBMS along with the other tables.
Assuming 100M total users, and 10M total workers, we are looking at (very roughly speaking)

(100M x 1200) `User` + (10M x 1200) `Worker` + (10M x 100 x 5) `WorkerServiceLocations` + (10M x 365 x 5 x 50) `Orders`
~= (120000 + 12000 + 5000 + 900000)M bytes ~= **1 PB**.

This is the overall storage requirement over 5 years.
While this is large, this isn't something that will be updated at a high velocity (like the stream of heavy orders).
It should be possible to store this data in a RDBMS.
Furthermore, from the above calculations, we see that the largest component here is the orders data pieces.
Once orders are completed, we could have a setup that moves them to a cold storage/DB backup, thus immediately freeing up storage space for new orders.
Queries for past orders will then have to perform lookup in the cold storage DB (which could be slower, but isn't mission-critical to our application so it should be OK).
Thus, we will use a KV store (like Redis) for storing the `order_id -> description` mapping, and use an RDBMS (like Postgres) to store the relational bits of our data.

For storing the search index information, we can leverage a document-style search index like Solr.
Each document has fields, and the ones of particular importance for our application are: `worker_id`, `rating`, `price`, `lat`, `lng`, `radius`.
A query to the search index could look like this:

```text
fq: price <= $max_price AND $loc in Circle(base_loc, radius) AND type = $type
```
The response looks like:

```text
response: [
    { worker_id:1, name:"John Doe", avg_rating:4.3, ratings:122, price_per_hr:56 }, 
    { worker_id:5, name:"Jess Min", avg_rating:4.7, ratings:100, price_per_hr:60 },
    ...
]
```
See [Solr documentation](https://solr.apache.org/guide/6_6/spatial-search.html) for more details on exact query syntax.


<figure class="blog-fig">
  <img src="/assets/images/task-hiring-digging-deeper.png">
  <figcaption>Figure 2. Deep dive into system architecture</figcaption>
</figure>

Fig. 2 shows the zoomed in version of our system.
The frontend accepts the `getCandidateWorkers` request and parses it to generate a request for the search service.
It then issues this request, and returns the response to the end-user (the list of candidate workers).

The frontend also accepts the `submitOrder` and `submitResponse` requests from the user and worker respectively.
Upon `submitOrder`, a POST request is sent to the order processing service.
This service is also stateless, so it can independently be scaled based on increasing/decreasing #orders being received per day.
The entities DB is our RDBMS store that stores relations.
The KV store is used to store (potentially lengthy) order descriptions.
Upon submitting an order, a notification can be sent to the worker to consider the work order.

The `submitResponse` flow is similar to `submitOrder`.
It ends up modifying the `Orders` table row for the `order_id` for which the worker wants to make a decision on.
Whatever the decision ('yes' to accept, or 'no' to reject), this is communicated to the user also via the notifications service.

Finally, there is a `submitRating` flow that is not shown in fig. 2 above, but its path is very similar to the `submitOrder` flow, just that it modifies the `Worker` table.
Note that `submitRating` must accept a `user_id`, `worker_id`, `rating` _and_ an `order_id` as parameters.
In other words, users should only be able to rate workers who have worked for them on an order in the past.
To semantically isolate orders processing from rating submissions, we should set up a separate service for this API, say, a ratings processing service.
This service as part of its due diligence, checks for whether an `order_id` provided has been marked completed, before updating the worker rating.

Notifications processing needs to happen asynchronously to order submissions and updates.
Whenever an order is submitted, or its status updated by the worker, an event is sent to a persistent queue.
Having a queue in between here allow for the system to remember notification events to send out when users are offline.
These events are eventually consumed by a worker pool, which then unpack the event data, and send a push notification to an external push notification service, based on user device details (e.g. APN for Apple devices).
We will resist diving any deeper into push notifications, as these systems themselves can be quite complex and require quite some time to dive into.

The entire system can use HTTP REST API endpoints to communicate between the various services, as shown in the figure as well.

## Further considerations

It is very likely that you are at the end of your 45 minute interview by this point.
With whatever handful of minutes you may have left, there are some important flows below that may be worth giving a quick once-over.

1. How does new worker registration happen? How about worker profile/preference updates?
2. How can we support a work order cancellation/postponing flow?
3. How can we set up work order date/time updates made by the worker? Perhaps a worker is not able to arrive exactly at the requested time but 15 minutes later, and perhaps this is fine by the user as well.
4. How can we support order reminders for the worker? I.e. if a worker does not respond to a work order within X hours, the system sends a push notification about it to the worker.
5. How can we set up logging and monitoring of this entire system?

Also remember that while most of this blog is 'prescriptive', you must always discuss tradeoffs when discussing with the interviewer.
Some tradeoff example questions to touch upon:

1. Why implement stateless services? What happens if we use sticky sessions?
2. Why a KV store for storing descriptions? Why not compress the description and store in RDBMS? Which approach is more extensible (e.g. to image sharing)?
3. Why use a search index? What alternatives do we have here?
4. Why use a queue in between notifications workers and the order processing service? Why not directly call the notifications service from within the order processing service?
