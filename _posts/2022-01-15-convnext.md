---
layout: post
author: Nishant Kelkar
title: A review of ConvNext networks
tags: paper-review
---

Today we will review the [ConvNext](https://arxiv.org/abs/2201.03545) paper.
It is assumed that you know what Transformers and ConvNets are.
If not, for Transformers I would recommend Jay Alammar's excellent [blog](https://jalammar.github.io/illustrated-transformer/) on the topic.
For ConvNets, there are many resources, for example [this](https://d2l.ai/chapter_convolutional-neural-networks/index.html) one from `d2l.ai`.

### Background on ConvNets, Transformers, and the problem statement

_Can ConvNets achieve Transformer level accuracy on various Computer Vision tasks?_

The paper describes the central operation of 'the convolution' as a fundamental desirable property in Computer Vision.
The rise of ConvNets in tasks like image classification and object detection can be attributed to 2 properties of these networks, which are also referenced as 'inductive biases' of these networks: translation equivariance, and computation sharing due to sliding-windows.
Translation equivariance means that the operations `translate(OP(x))` and `OP(translate(x))` yield the exact same result.
`OP` here is what we call the translation equivariant operation.
The computation sharing also makes these networks attractive due to their efficiency.

The [Vision Transformer](https://arxiv.org/pdf/2010.11929.pdf) (ViT) while great for image classification of low-resolution images (like ImageNet for example), suffers from a quadratic time complexity (quadratic in the #patches generated) and so is not so great for tasks using high-resolution images.
It also does not have the same inductive biases as the ConvNet, i.e. no sliding-window and therefore no translation equivariance guarantees.

[Swin Transformers](https://arxiv.org/pdf/2103.14030.pdf) improve upon ViTs by employing a hybrid approach.
Instead of computing self-attention like the ViT globally across the whole image, Swin Transformers instead compute self-attention in bounded regions within the image.
Deeper layers of the network contain Transformer blocks that work on larger bounded regions.
In the Swin paper, each layer `l + 1` uses a region of size `2H x 2W` where `H` and `W` are the height and width of the region at layer `l`.

The general observation that the authors make here is that Transformers are better than ConvNets when it comes to _scaling_ - with larger dataset and model sizes, Transformers tend to do better.
The authors proceed to start with a simple ResNet-50 ConvNet, and gradually 'modernize' it by looking carefully at what Transformers are doing, to improve the ConvNets performance on image classification, object detection, and semantic segmentation.
They dub this 'next generation' ConvNet as a `ConvNext`.

### The proposed solution

The following figure summarizes the key modifications made by the authors in this paper to vanilla ResNet-50 and ResNet-200 models.

<figure class="blog-fig">
  <img src="/assets/images/convnext-modifications.png" width="40%" height="40%">
  <figcaption>Figure 1. The evolution of the ConvNext network.</figcaption>
</figure>

Note that the baseline model (the bar at the very top, 78.8% acc) is a ResNet-50/200 trained with the training technique used with ViT and Swin (e.g. a new optimizer called `AdamW`).
This baseline accuracy of 78.8% is itself a significant improvement over the ResNet-50 literature accuracy of 76.1%.

Data augmentation is used, specifically Cutmix, Mixup, RandAugment, and Random Erasing.
Regularization is also employed -- specifically the Stochastic Depth and Label Smoothing techniques (checkout the paper reference for more details).

The initial ResNet-50 'stem cell' (a compute block responsible for initial input image processing) was replaced with a 'patchify' layer that simply cuts up the input image into $4 \times 4$ non-overlapping patches and convolves each patch with a learnable kernel.


### Some key results

### Personal takeaways

All in all this was definitely an intriguing paper.
The authors document a journey of sorts, picking up tid bits of knowledge nuggets from previous ConvNets from the past decade (e.g. MobileNetV2, ResNeXt, etc.) and incrementally improve the baseline ResNet-50's performance with each enhancement.
Firstly, I think just this process itself, of incrementally improving a network and checkpointing performance along the way, is of great value.
More papers should start doing this, as it helps readers to understand the thought process behind obtained results!

Secondly, it was quite surprising to me that ConvNext was able to beat the Swin Transformer architecture at the ImageNet-1K task, for all model sizes, both in the trained-from-scratch and ImageNet-22K pretrained settings.
This is great news because now we can retain all the great model explainability work done for ConvNets, and yet still achieve Transformer level performance.
