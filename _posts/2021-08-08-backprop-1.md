---
layout: post
author: Nishant Kelkar
title: An introduction to backprop - intuition
tags: machine-learning deep-learning
---

Backpropagation (or backprop for short) is an application of dynamic programming. The goal of this blog is to introduce backprop and to discuss some basic properties of the functions used in what we will know as computation graphs.

### Derivative chain rule

Backprop is a technique to compute derivatives first named and proposed in the 1986 Rumelhart et. al. [paper](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf).

The most important property of derivative calculus that backprop leverages is the chain rule, so it makes sense to start there. Suppose we have two functions $T = f(y)$ and $y = g(x)$ where $x$ is a real-valued variable.

Now one may ask the question: what is the derivative of $T$ with respect to $x$?
Based on the above equations, it seems it should be straightforward to compute $\frac{dT}{dy}$ and $\frac{dy}{dx}$.
But how do you compute $\frac{dT}{dx}$?
The chain rule says we can compute it as follows:

$$
\begin{align}
    \frac{dT}{dx} &= \frac{dT}{dy} \times \frac{dy}{dx} \\
    &= f'(y) \times g'(x)
\end{align}
$$

This means, if we can compute $\frac{dT}{dy}$ and $\frac{dy}{dx}$ individually, then we can compute $\frac{dT}{dx}$ by using the previous computations.
A crude and simple way to remember this formula is to think that the $dy$ in the denominator of $\frac{dT}{dy}$ and in the numerator of $\frac{dy}{dx}$ "cancel out".

### Simple computational graphs

You can also draw a graph for the above process.
We can first make something called a "computation graph" out of the formulae for $T$ and $y$.
This graph would look something like the below:

<figure class="blog-fig">
  <img src="/assets/images/backprop-intro-1.jpeg">
  <figcaption>Figure 1. A simple computation graph</figcaption>
</figure>

An arrow from A to B here means "B depends on A being computed first".
We can see that computing $y$ depends on the availability of $x$, and computing $T$ in turn depends on $y$ being available.
The ultimate function that we want to take a derivative of is $T$ here.
So at each node $k$ in this simple graph, we can ask the question: what is the derivative of $T$ with respect to $k$?
At $T$, we need $\frac{dT}{dT} = 1$, so this is trivial.
At $y$, we need $\frac{dT}{dy} = f'(y)$, something we can compute analytically.
At $x$, we need $\frac{dT}{dx}$ which can be computed by first computing $\frac{dy}{dx}$, and then _reusing_ $\frac{dT}{dy}$ from a previous computation.
This last part is where computational graphs have a dynamic programming flavor to them.

What other patterns of computational graphs can we come up with?
Consider the following:
$$
\begin{align*}
    T = f(y, z)
    y = g(x)
    z = h(x)
\end{align*}
$$
How would the computation graph look for this setup?
Similar to above, we start right-to-left, placing T at the far right of the graph (the value we want).
T depends on y and z, each of which depend on x.
We then draw the nodes for T, y, z, and x, and connect them up with arrows based on the semantics discussed above: an arrow from A to B means "B depends on A".
The resulting graph will look like this:

<figure class="blog-fig">
  <img src="/assets/images/backprop-intro-2.png">
  <figcaption>Figure 2. A slightly more complicated computation graph</figcaption>
</figure>

We notice an inmportant property of the derivative with respect to $x$ here.
$x$ "feeds in" to 2 variables here, $y$ and $z$.
Thus when computing the derivative of $T$ w.r.t. $x$ we need to consider the contribution coming from $y$ and $z$ individually, and add them up.
In an equation, this would look like:

$$
\begin{align*}
    \frac{dT}{dx} = (\frac{\delta T}{\delta y} \times \frac{dy}{dx}) + (\frac{\delta T}{\delta z} \times \frac{dz}{dx})
\end{align*}
$$

Again, in each of the additive terms, think of $\delta y$ and $dy$ and $\delta z$ and $dz$ as "canceling out" (although in reality this isn't what is happening).
In general, if $T$ is a function of variables $v_{1},v_{2},...,v_{n}$, then the derivative of $T$ w.r.t. $x$ is given by:

$$
\begin{align*}
    \frac{dT}{dx} = (\frac{\delta T}{\delta v_{1}} \times \frac{dv_{1}}{dx}) + (\frac{\delta T}{\delta v_{2}} \times \frac{dv_{2}}{dx}) + ... + (\frac{\delta T}{\delta v_{n}} \times \frac{dv_{n}}{dx})
\end{align*}
$$

### Properties of functions used in computational graphs

Now let's focus in on the types of functions we can use in these computational graphs, i.e. the functions like $f(., .)$, $h(.)$, and $g(.)$ in the graph above.
Of course, it seems that for the domain of possible input values, the best case would be that the function is everywhere differentiable.
However, it turns out that for neural networks, this condition does not hold.
As we will see in later blog posts, neural networks add in a little bit of "magic" by including non-linear transformations, usually in the form of $f(x) = max(0, x)$.
This is called the ReLU function, short for "Rectified Linear Unit".
If $T = max(0, x)$, what is $\frac{dT}{dx}$?
Strictly speaking, it is not defined at x = 0, since the function is not differentiable at that point.

<figure class="blog-fig">
  <img src="/assets/images/relu.png">
  <figcaption>Figure 2. A plot of $T = max(0, x)$</figcaption>
</figure>

However, think about when such a function is implemented on a computer in a real life situation.
$x$ is probably some intermediate output in a neural network.
So first of all, its chances of being exactly 0 are extremely slim.
And secondly, even if it is exactly 0, we can simply adopt the convention that $\frac{dT}{dx} = 0$ when $x = 0$.
Even though this simplistic assumption may be cringeworthy to some (sorry @all mathematicians out there reading this!), in practice there aren't any negative effect observed as a result of this assumption (you could also make the assumption that $\frac{dT}{dx} = 1$ when $x = 0$ and that would work fine too).

### Summary

Here is what we covered today:

1. The derivative chain rule that helps us reuse computations made at a previous point in time

2. Simple computation graphs, where an individual node is an input to 1 other node, or to 2 other nodes (or in general, to $n$ other nodes)

3. The desired characteristics of a function so that it may be used in a computation graph (i.e. differentiable over the domain of its valid inputs) and how to deal with non-linear functions that are common in neural network literature but that do not have these desired properties. We also looked at an example of such a function, $T = max(0, x)$

Next up, we will examine what happens when some of these variables (e.g. $x$ in our examples above) are vector valued!
